# Camera 2D Object Tracking: Camera & Lidar Fusion




### FP.1 Match 3D Objects
Implemented the method "matchBoundingBoxes". The alogorithm is to related bounding boxes in previous and current frame, by using the knowledge from feature matching.
The first step is to generate a 2D container, the col and row are the bounding box index from previous and current frame respectively. Then loop through all matched features (in `cv::DMatch`), check its `queryIdx` & `trainIdx` to get the corresponding bounding box's index which the keypoint belongs to, and increase the 2D container accordingly. After construct the 2D container, complete the `map<int, int>` but choosing the best matching which has the highest number of keypoint correspondences from each row.


### FP.2  Compute Lidar-based TTC
To compute the time-to-collision based on Lidar, this part would follow the Constant Velocity Model and calculate the TTC from:
```cpp
TTC = (d1 * dT) / (d0 - d1);
```
Apart from preparing the Lidar Point clouds, it is important to make it robust against outliers which could results in faulty estimation. To tackle the outliers problem, there are mant approaches on hands, such as PCA, DBSCAN... but in this project a statistic method has been leveraged. The algorithm is simple: 
1. calucate `mean` & standard derivative `std`.
2. choose a threshold as `mean - n * std` for outlier removal. (n = 1, 2, or 3, in this project n = 2)
<img src="images/outlier.png" width="1000" height="150" />
After outlier removal, the closest lidar point to the ego vehicle is picked for TTC calculation. 


### FP.3 Associate Keypoint Correspondences with Bounding Boxes
Find all keypoint matches that belong to each 3D object, by checking whether the corresponding keypoints are within the region of interest in the camera image. One problem can be addressed as outliers removal. From the forum, this can be solved by leveraging mean of all the Euclidean distances between keypoint matches and then remove those are too far from the mean. 

In this project, a RANSAC based appraoch has been developed. The idea is to by calculateing the prespective relationship (Homography matrix)`cv::findHomography()` between keypoints in the bounding box from different frame, it can use RANSAC (`cv::RANSAC`) for inlier keypoints estimation and inlier matches as well.

<img src="images/matches.png" width="1000" height="370" />

Once all the keypint matches within the bounding boxes, then compute TTC estimate.


### FP.4 Compute Camera-based TTC
Compute the time-to-collision in second for all matched 3D objects using only keypoint correspondences from the matched bounding boxes between current and previous frame.

To compute the time-to-collision based on Lidar, this part would follow the Constant Velocity Model and calculate the TTC from:
```cpp
TTC = -dT / (1 - meanDistRatio);  

TTC = -dT / (1 - medianDistRatio);   // with outlier
```
This part in the project is using the previous solution for TTC estimate problem, which is based on median distance ration. 

<img src="images/results.png" width="500" height="185" />

The 2D feature matching is completed in previous project, after integrating that code and the Lidar-Camera fusion TCC estimate system could be ready for performance validation. 



### FP.5 Performance Evaluation 1
Here are examples where the TTC estimate of the Lidar sensor does not seem plausible.

<img src="images/evaluation1.png" width="1000" height="363" />

To analyze these plausible cases,the table below has been generated by TTC, Minimum x-distance and decremental in Minimum x-distance from each frame.
 
| Frame | LiDAR ttc (s) | min_x (m)  | decremental (m) |
|:-----:|:-------------:|:----------:|:---------------:|
|   0   |      n/a      | 7.974      |     n/a         |
|   1   |    12.9722    | 7.913      |     0.061       |
|   2   |    12.2640    | 7.849      |     0.064       |
|   3   |    13.9161    | 7.793      |     0.056       |
|   4   |    14.8865    | 7.741      |     0.052       |
|   5   |    12.1873    | 7.678      |     0.063       |
|   6   |     7.5020    | 7.577      |     0.101       |
|   7   |    34.3404    | 7.555      |     0.022       |
|   8   |    18.7875    | 7.515      |     0.040       |
|   9   |    15.8894    | 7.468      |     0.047       |
|  10   |    13.7297    | 7.414      |     0.054       |    
|  11   |    10.4914    | 7.344      |     0.070       |
|  12   |    10.1000    | 7.272      |     0.072       |
|  13   |     9.2231    | 7.194      |     0.078       |
|  14   |    10.9678    | 7.129      |     0.065       |
|  15   |     8.0942    | 7.042      |     0.087       |
|  16   |     8.1391    | 6.963      |     0.215       |
|  17   |    10.2926    | 6.896      |     0.079       |
|  18   |     8.3098    | 6.814      |     0.082       |

Look at the obviously inaccurate cases, such as frame 6-8: the distance decremental are varying dramatically. Becasue the TTC lidar is calculating based on Constant Velocity Model:
```
d(t+t') - d(t) = v0 * t'
```
Firstly looking at the decrements are relatively stable during a constant time interval, such as frame 11 to 13, the TTC was decreasing near linearly because the distance to preceding vehicle is reduced near linearly. So it is in the CVM framework.  
But in practice the CVM assumption is too ideal to get remained. Thus, when the decrements in distance became smaller (preceding vehicle is decelerating), based on the CVM formular the TTC is calculated even much larger (such as in frame 7, 16), meanwhile, the preceding vehicle is accelerating so that the decrements getting larger but the TTC is calculated even smaller. Obviously it does not make sense in real world. 



### FP.6 Performance Evaluation 2
Run several detector / descriptor combinations and look at the differences in TTC estimation. Find out which methods perform best and also include several examples where camera-based TTC estimation is way off. As with Lidar, describe your observations again and also look into potential reasons.





